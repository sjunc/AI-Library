{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpgMT44V84EUBMntPedEM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjunc/AI-Library/blob/main/class/W06_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6주차 실습 단어유추"
      ],
      "metadata": {
        "id": "EOLgOdKB7Jb6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pLQbICDWm-dM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "# drive.mount(\"/content/drive\") # colab과 google drive 연결"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string # 특수문자 등을 처리하는 텍스트 처리용 도구\n",
        "\n",
        "# 드라이브에서 뉴스 기사 데이터 읽어오기\n",
        "df = pd.read_csv(\"ArticlesApril2017.csv\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWnvVdN1nH3J",
        "outputId": "00610baf-120e-45e1-ece0-ddf92b444f5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
            "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "  def clean_text(self, txt):\n",
        "    # 모든 단어를 소문자로 바꾸고 특수 문자 제거\n",
        "    # Hello, World! - >  hello world\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # punctuation 구두점\n",
        "    return txt\n",
        "\n",
        "  # 초기화\n",
        "  def __init__(self):\n",
        "    all_headlines = []\n",
        "\n",
        "    # 모든 헤드라인의 텍스트를 불러옴\n",
        "    # 여러 csv 파일 중에 \"Articles\"란 이름이 포함된 csv 파일만 찾아서 headline 컬럼만 뽑아서 리스트에 저장\n",
        "    # glob — glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환\n",
        "    for filename in glob.glob('*.csv'):\n",
        "      if 'Articles' in filename:\n",
        "        article_df = pd.read_csv(filename)\n",
        "\n",
        "        # 데이터셋의 headline의 값을 all_headlines에 추가\n",
        "        all_headlines.extend(list(article_df.headline.values))\n",
        "        break\n",
        "\n",
        "    # headline 중 unknown 값은 제거\n",
        "    # 결측치 제거하기 위함\n",
        "    all_headlines = [h for h in all_headlines if h != \"unknown\"]\n",
        "\n",
        "    # 구두점 제거 후 전처리된 문장 리스트 생성\n",
        "    # 이후 단어별 고유 인덱스를 지정할 Bag of Words (BOW) 사전 생성\n",
        "    self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "    self.BOW = {}\n",
        "\n",
        "    # 모든 문장의 단어를 추출해 고유번호 지정\n",
        "    for line in self.corpus:\n",
        "      for word in line.split():\n",
        "        if word not in self.BOW.keys():\n",
        "          self.BOW[word] = len(self.BOW.keys())\n",
        "\n",
        "    # 각 문장을 단어 인덱스 시퀀스로 변환한 후,\n",
        "    # 입력형태를 ([단어1, 단어2]) -> 출력형태 ([단어3])으로 바꿈\n",
        "    def generate_sequence(self, txt):\n",
        "      seq = []\n",
        "\n",
        "      for line in txt:\n",
        "        line = line.split()\n",
        "        line_bow = [self.BOW[word] for word in line]\n",
        "\n",
        "        #단어 2개를 입력으로, 그 다음 단어를 정답으로\n",
        "        data = [(([line_bow[i], line_bow[i+1], line_bow[i+2]])\n",
        "        for i in range(line_bow)-2)]\n",
        "\n",
        "        seq.extend(data)\n",
        "\n",
        "      return seq\n",
        "\n",
        "    # 모델의 입력으로 사용할 데이터\n",
        "    # 입출력의 쌍 만들기\n",
        "    self.data = self.generate_sequence(self.corpus)\n",
        "\n",
        "    # 데이터의 수 반환\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "    # 학습에 사용할 x, y 값을 리턴함.\n",
        "    # data: 단어 2개의 index,   label: 그 다음 단어(정답 단어)의 index\n",
        "    def __getitem__(self, i):\n",
        "      data = np.array(self.data[i][0]) # 입력 데이터\n",
        "      label = np.array(self.data[i][1]).astype(np.float32) # 정답 데이터\n",
        "\n",
        "      return data, label\n"
      ],
      "metadata": {
        "id": "kHUr3Ay7nuvN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# pytorch의 nn.Module을 상속받아 새로운 LSTM 기반 텍스트 생성 모델 정의\n",
        "# num_embeddings: 사용할 단어(토큰)의 총 개수\n",
        "# 예: BOW 안에 단어가 2000개 있음 -> num_embedding\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, num_embedding):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    # 희소 표현을 밀집 표현으로 만들기 위한 임베딩 층\n",
        "    # 임베딩 층을 지나면 16개 값이 있는 벡터로 변환 밀집표현\n",
        "    self.embed = nn.Embedding(\n",
        "        num_embeddings= num_embeddings, embedding_dim = 16)\n",
        "\n",
        "    # LSTM 을 정의\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=16, # 임베딩 벡터의 크기\n",
        "        hidden_size=64, # hidden state(출력)의 차원(크기)\n",
        "        num_layers=5,   # lstm을 5개를 쌓음\n",
        "        batch_first=True # 입력받는 shape: 배치가 먼저 오도록 맞춰줌 -> (배치사이즈 * 시퀀스길이=2 X 입력의 크기)\n",
        "    )# LSTM은 계층이 여러 개일 때, 맨 마지막 층의 출력만 forward() 결과로 출력됨. (1~4번째 LSTM은 hidden state를 직접 출력하지 않음)\n",
        "\n",
        "   # 분류를 위한 MLP 층\n",
        "    self.fc1 = nn.Linear(128, num_embeddings) # fc FC(Fully Connected Layer)\n",
        "    self.fc2 = nn.Linear(num_embeddings, num_embeddings) # 최종 출력\n",
        "\n",
        "    # 활성화 함수\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    # LSTM에 들어가는 입력 모양 [batch_size, seq_len, input_size]\n",
        "    # - batch_size: 한번에 처리하는 문장(데이터)의 개수 (예: 64)\n",
        "    # - seq_len: 한 문장에서 모델이 바라보는 단어의 개수 (예: 2개씩 입력)\n",
        "    # - input_size: 임베딩을 거친 단어 벡터의 차원 수 (예: 16차원)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 희소 표현을 밀집 표현으로 변환\n",
        "        x = self.embed(x) # 입력:  [batch, seq_len] -> 임베딩: [batch, seq_len, 16]\n",
        "\n",
        "        #LSTM 모델의 예측값\n",
        "        x, _ = self.lstm(x)\n",
        "        # LSTM 통과: 각 단어마다 64차원의 hidden state를 생성\n",
        "        # 출력 : [batch, seq_len, 64]\n",
        "\n",
        "        # 모든 시퀀스의 hidden state를 하나로 이어붙이기 (Flatten)\n",
        "        # [batch, seq_len X 64] -> [batch, 128] (seq_len 이 2일 경우)\n",
        "        x = torch.reshape(x, (x.shape[0], -1)) # [batch, seq_len X 64] -> Flatten\n",
        "\n",
        "        # MLP 통과 (예측값 생성)\n",
        "\n",
        "        x = self.fc1(x) # [batch, 128] -> [batch, 2000]\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x) # [batch, 2000] -> [batch, 2000]\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "J8jiVE5mr4qt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습을 진행할 프로세스 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dataset = TextGeneration()  # 커스텀 데이터셋 불러오기\n",
        "model = LSTM(num_embeddings = len(dataset.BOW)).to(device) #모델 정의\n",
        "loader = DataLoader(dataset, batch_size = 64) # 배치 단위로 데이터 나눔\n",
        "optim = Adam(model.parameters(), lr= 0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "  iterator = tqdm.tqdm(loader)  # tqdm 프로세스바\n",
        "  for data, label in iterator:\n",
        "    # 기울기 초기화\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # 모델의 예측값\n",
        "    pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
        "\n",
        "    # 정답 레이블은 long 텐서로 반환해야 함\n",
        "    # (퀴즈) CrossEntropyLoss가 long 정수를 쓰는 이유는?\n",
        "    loss = nn.CrossEntropyLoss()(\n",
        "        pred, torch.tensor(label, dtype=torch.long).to(device))\n",
        "\n",
        "    # 오차 역전파\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    iterator.set_description(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"lstm.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "D2ItsT_Gs-BO",
        "outputId": "fb34a857-c58f-4130-b106-13d7b354f3c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TextGeneration' object has no attribute 'generate_sequence'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-51960e730588>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 학습을 진행할 프로세스 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 커스텀 데이터셋 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#모델 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 배치 단위로 데이터 나눔\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-88f4209d48a5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# 모델의 입력으로 사용할 데이터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# 입출력의 쌍 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# 데이터의 수 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TextGeneration' object has no attribute 'generate_sequence'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XE4LM3CDuhpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rWN56x2uh4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top-k 확률 높은 k개의 단어 중 선택\n"
      ],
      "metadata": {
        "id": "1EYHFgtmuiVF"
      }
    }
  ]
}